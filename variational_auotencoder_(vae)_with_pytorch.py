# -*- coding: utf-8 -*-
"""Variational_AuotEncoder_(VAE)_with_PyTorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTSdI1D8Z5PX-9Fy3C2oxjyCZPW-UNB2

# Contents
-  Basic concept

- Building the VAE model:
 - Begin with a simple model architecture.
 - Adjust and experiment with different parameters iteratively.
 - Progressively enhance model complexity.

- Datasets practiced on:
 - MNIST.
 - CIFAR-10.
 - 102 Category Flower Dataset (CNN).

- Results, observations, questions.

# 1. Understanding the Basics of VAE

VAE is a generative model that learns the latent representation of data and can generate new data.

VAE uses probability and statistics to ensure that the generated data is similar to the original data.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Variational Autoencoder (VAE) Components

### 1. Encoder
- **Input:** Data point $x$
- **Output:** Mean $\mu$ and log variance $log(\text{var})$
- **Task:** Transform the input data to distribution parameters in the latent space.


### 2. Reparameterization
- **Task:** Enable differentiable sampling in VAEs for gradient descent optimization.
- **Sampling:** Draw $ \epsilon $ from a standard normal distribution: $ \epsilon \sim N(0, 1) $.
- **Compute latent variable $ z $:** Use the sampled $ \epsilon $ and the encoder outputs mean $ \mu $ and standard deviation $\sigma $:
  $ z = \mu + \epsilon \times \sigma$

### 3. Decoder
- **Input:** Latent variable $ z $
- **Output:** Reconstructed data $ \hat{x} $
- **Task:** Transform the point in latent space back to data space.


### 4. Loss Function

The VAE employs a specialized loss function that consists of two main components:

$$
L = \color{red}{\text{KL Divergence}} + \color{blue}{\text{Reconstruction Loss}}
$$

- **KL Divergence (KLD)**: Quantifies the divergence of the learned latent variables from a standard normal distribution. The VAE aims to ensure that its latent space representations approximate this standard normal distribution.

$$
\color{red}{\text{KL Divergence}} = -0.5 \times \sum(1 + \log(\text{var}) - \mu^2 - \text{var})
$$


- **Reconstruction Loss (BCE)**: Measures the difference between the original data and the reconstructed data.

$$
\color{blue}{\text{Reconstruction Loss}} = ||x - \hat{x}||^2
$$

- **Task:** Provide VAE with a clear optimization goal to guide parameter updates during training, ensuring accurate input reconstruction and a structured latent space.

# 2. VAE implementation in PyTorch

### Step 1. Define the VAE Model

- Initialization: Set up the architecture of the VAE with specified input dimensions, hidden layer dimensions, and latent dimensions. \
- Encoder: Compresses the input data into a latent space representation. It consists of linear layers followed by ReLU activations. \
- Decoder: Reconstructs the original input from the latent space representation. Ends with a Sigmoid activation due to the MNIST data range being [0, 1].
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dims=[400], latent_dim=20):
        super(VAE, self).__init__()

        # Define the encoder part
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]), # Linear transformation
            nn.ReLU(), # Activation function
            nn.Linear(hidden_dims[0], 2 * latent_dim)  # Split into mean and variance
        )

        # Define the decoder part
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dims[0]), # Linear transformation
            nn.ReLU(), # Activation function
            nn.Linear(hidden_dims[0], input_dim), # Linear transformation
            nn.Sigmoid()  # Activation function: since MNIST data range is [0, 1]
        )

    # Reparameterization trick
    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance) # Standard deviation
        epsilon = torch.randn_like(std_dev) # Random noise
        return mean + epsilon * std_dev

    # Forward pass
    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)  # Split h into mean and variance
        z = self.reparameterize(mean, log_variance) # Get a sample from the latent space
        return self.decoder(z), mean, log_variance

"""The code defines a Variational Autoencoder using PyTorch. It consists of an encoder and a decoder. The encoder transforms the input data into a latent representation, which is then reconstructed back to the original data by the decoder. The reparameterization trick is used to allow gradients to flow back through the sampling process.

### Step 2. Loss Function
"""

# Loss components
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# VAE loss
def vae_loss(reconstructed_x, x, mu, log_var):
    BCE, KLD = vae_loss_components(reconstructed_x, x, mu, log_var)
    return BCE + KLD

"""### Step 3. Data Loading - MNIST Dataset"""

from torchvision import datasets, transforms

# Define batch size
batch_size = 64

transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""### Step 4. Model, Optimizer Setup
Initialize the VAE model and define the optimizer.
"""

learning_rate = 0.001

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = VAE().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

"""### Step 5. Training Loop
Train the VAE using the MNIST training dataset.
"""

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, 784).to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

"""### Step 6. Model Evaluation
Evaluate the VAE on the MNIST test dataset.
"""

model.eval()
test_loss = 0
with torch.no_grad():
    for data, _ in test_loader:
        data = data.view(-1, 784).to(device)
        reconstructed, mu, log_var = model(data)
        test_loss += vae_loss(reconstructed, data, mu, log_var).item()

print(f"Test Loss: {test_loss / len(test_loader.dataset):.4f}")

"""### Step 7. Visualizing Reconstructions
Visualize how well the VAE reconstructs images from the test dataset.
"""

import matplotlib.pyplot as plt

with torch.no_grad():
    sample = torch.randn(64, 20).to(device)
    sample = model.decoder(sample).cpu()

fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
plt.show()

"""## 2.1 Basic VAE Model
Fully Connected VAE / Linear VAE

### Detailed Evaluation of VAE Model Loss Components

**Benefits of the Modified Evaluation:**

- Provides a deeper understanding of model performance by separating reconstruction and KL divergence losses.
- Aids in model debugging and optimization by highlighting specific areas for improvement.
"""

# 1. Initialization
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dims=[400], latent_dim=20):
        super(VAE, self).__init__()

        # Define the encoder part
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]), # Linear transformation
            nn.ReLU(), # Activation function
            nn.Linear(hidden_dims[0], 2 * latent_dim)  # Split into mean and variance
        )

        # Define the decoder part
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dims[0]), # Linear transformation
            nn.ReLU(), # Activation function
            nn.Linear(hidden_dims[0], input_dim), # Linear transformation
            nn.Sigmoid()  # Activation function: since MNIST data range is [0, 1]
        )

    # Reparameterization trick
    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance) # Standard deviation
        epsilon = torch.randn_like(std_dev) # Random noise
        return mean + epsilon * std_dev

    # Forward pass
    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)  # Split h into mean and variance
        z = self.reparameterize(mean, log_variance) # Get a sample from the latent space
        return self.decoder(z), mean, log_variance

# 2. Loss function
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# 3. Data loading
from torchvision import datasets, transforms

# Define batch size
batch_size = 64

transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 4. Model, Optimizer Setup

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE().to(device)

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# 5. Training
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, 784).to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# 6. Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.view(-1, 784).to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

"""**Results Insights:**

- Total Test Loss (107.3149): Overall model performance on the test set.
Reconstruction Loss (BCE: 82.3565): Indicates the model's capability in data reconstruction. Higher values suggest potential reconstruction issues.
- KL Divergence (KLD: 24.9584): Measures how closely the latent space aligns with a standard normal distribution.

**Conclusion:** The model has a larger loss in data reconstruction compared to maintaining latent space distribution. Emphasis may be needed on improving reconstruction quality in further optimizations.
"""

# # 7. Visualization
# import matplotlib.pyplot as plt

# with torch.no_grad():
#     sample = torch.randn(64, 20).to(device)
#     sample = model.decoder(sample).cpu()

# fig = plt.figure(figsize=(10, 10))
# for i in range(64):
#     plt.subplot(8, 8, i + 1)
#     plt.imshow(sample[i].reshape(28, 28), cmap="gray")
#     plt.axis("off")
# plt.show()

"""## Hyperparameter Impact on Model Performance

- Batch Size (32, 64, 128):
 - 32: Allows for more frequent weight updates; Might help with faster convergence; Risk of unstable training and suboptimal local minima.
 - 128: Leads to more stable training; Might require more iterations to converge; Better utilization of parallel processing capabilities.

- Epochs (5, 10):
 - Helps in achieving better performance on training data.
 - Risk of overfitting, especially without adequate regularization or early stopping.

- Learning rate (0.001, 0.01):
 - Initial Selection: The choice of the initial learning rate is crucial for model convergence.
 - Default for Adam: While the default learning rate for Adam is 0.001, which may not be optimal for all tasks.
 - High Learning Rate: Can lead to unstable training or divergence.
 - Low Learning Rate: Might result in slow convergence or getting stuck in poor local minima.
 - Tuning?

### Model Optimization Methods:

- Adjust Hidden Layers: Experiment with different dimensions and numbers of hidden layers, e.g., [512, 256] or [300].
- Modify Latent Space Dimension: Altering this might affect reconstruction quality and interpretability of latent representations.
- Regularization: To prevent overfitting, add L1 or L2 regularization on model weights.
- Optimizer & Learning Rate: Consider using different optimizers like SGD or RMSprop, and adjust learning rates or decay strategies.
- Complex Network Structures: Explore using convolutional layers in place of fully connected layers, especially for image data.
- Early Stopping: Halt training if validation loss doesn't decrease significantly to prevent overfitting.
- Data Augmentation: Enhance training data with techniques like random rotations, scaling, and cropping to improve model generalization.
- Use Validation Set: Adjust hyperparameters with a validation set and evaluate final model performance on a test set.

The 'Fully Connected VAE' or 'Linear VAE' is typically considered the basic or standard version of VAE. In this architecture, the model employs fully connected layers (dense layers) instead of convolutional layers for encoding and decoding. This structure is particularly suitable for handling data that has been flattened, such as datasets like MNIST.

## 2.2 Convolutional (CNN) VAE

The choice between a "Linear VAE" (or "Fully Connected VAE") and a "CNN VAE" often depends on the nature and complexity of the data.

**MNIST:**

- The MNIST dataset contains grayscale images of size 28x28 pixels. Given its simplicity and small size, a linear VAE can often achieve reasonable results.
- However, using a CNN VAE can further improve performance, as convolutional layers are specifically designed to handle spatial hierarchies and patterns in images.

**CIFAR-10:**

- CIFAR-10 contains color images of size 32x32 pixels across 10 different classes. These images are more complex than those in MNIST.
- A linear VAE can be applied to CIFAR-10, but it may not capture the complexities and spatial relationships of the dataset as effectively as a CNN VAE. Using convolutional layers would likely result in better performance in terms of reconstruction and latent space representation.

To adapt the linear VAE to use convolutional layers, I'll make the following changes:

- 1. Modify the encoder to use convolutional layers instead of linear layers. This will extract spatial features from the input images.
- 2. Modify the decoder to use transposed convolutional layers to upscale and reconstruct the original image.
- 3. Adjust the forward method and other parts of the code to accommodate the change in data dimensions due to the use of convolutional layers.
- 4. Modify the data preparation in the training loop to remove the reshaping of the image to a flat vector since we're using convolutional layers.
Here's the updated code for a CNN VAE with MNIST:
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

class VAE(nn.Module):
    def __init__(self, latent_dim=20):
        super(VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1), # Output: 32x14x14
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # Output: 64x7x7
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64*7*7, 2 * latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64*7*7),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance)
        epsilon = torch.randn_like(std_dev)
        return mean + epsilon * std_dev

    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mean, log_variance)
        return self.decoder(z), mean, log_variance


# Loss components
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# VAE loss
def vae_loss(reconstructed_x, x, mu, log_var):
    BCE, KLD = vae_loss_components(reconstructed_x, x, mu, log_var)
    return BCE + KLD

# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE().to(device)

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)


# Training Loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)  # No reshaping here
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# Visualizing Reconstructions
import matplotlib.pyplot as plt

with torch.no_grad():
    sample = torch.randn(64, 20).to(device)
    sample = model.decoder(sample).cpu()

fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
plt.show()

"""#### Possible methods to improve the model

- Model Complexity: Enhance the model by adding more convolution layers or increasing the number of channels per layer.
- Loss Weights: Adjust the balance between reconstruction loss and KL divergence by scaling the KL term with a factor, which could be greater or less than 1.
- Learning Rate & Optimizer: Experiment with different learning rates or try other optimizers like RMSprop.
- Train Longer: Sometimes the model might need more epochs to converge properly.
- Advanced Techniques: Consider using advanced VAE variants, like beta-VAE, to fine-tune the balance between reconstruction and KL divergence.

**CIFAR-10:**

- CIFAR-10 contains color images of size 32x32 pixels across 10 different classes. These images are more complex than those in MNIST.
- A linear VAE can be applied to CIFAR-10, but it may not capture the complexities and spatial relationships of the dataset as effectively as a CNN VAE. Using convolutional layers would likely result in better performance in terms of reconstruction and latent space representation.

Visualize some sample images from the CIFAR-10 dataset using matplotlib
"""

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# Define data transformations - Convert image data to tensors and normalize
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# Load training data
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

# Define a function to show images
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get some random training images using a data loader
trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True)

dataiter = iter(trainloader)
images, labels = next(dataiter)

# Show images
imshow(torchvision.utils.make_grid(images))
# Print labels
classes = trainset.classes
print(' '.join('%10s' % classes[labels[j]] for j in range(5)))

"""### Attempt 1:"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

# Define the CNN VAE model
class CNN_VAE(nn.Module):
    def __init__(self, latent_dim=256, channels=3):
        super(CNN_VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(channels, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, latent_dim * 2)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (128, 4, 4)),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance)
        epsilon = torch.randn_like(std_dev)
        return mean + epsilon * std_dev

    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mean, log_variance)
        return self.decoder(z), mean, log_variance

# Loss components
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# VAE loss
def vae_loss(reconstructed_x, x, mu, log_var):
    BCE, KLD = vae_loss_components(reconstructed_x, x, mu, log_var)
    return BCE + KLD

# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_VAE().to(device)

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training Loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# Visualizing Reconstructions
import matplotlib.pyplot as plt

with torch.no_grad():
    sample = torch.randn(64, 256).to(device)  # change to match the latent_dim
    sample = model.decoder(sample).cpu()


fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].permute(1, 2, 0))  # change the order of dimensions to match the CIFAR-10 format
    plt.axis("off")
plt.show()

"""### Attemp 2: Enhanced Convolutional VAE for CIFAR-10
- Encoder:
 - Added multiple convolutional layers for better feature extraction.
 - Increased channel sizes to capture more intricate patterns.
 - Utilized ReLU activations after each convolution.
- Decoder:
 - Introduced Convolutional Transpose layers (ConvTranspose2d) to upsample and reconstruct the image.
 - Used Sigmoid activation in the final layer to ensure pixel values are between 0 and 1.

- Data Handling:
No need to reshape data to a vector of 784 elements; the convolutional layers can handle 2D images.
"""

class VAECNN(nn.Module):
    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):
        super(VAECNN, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Flatten()
        )

        self.fc1 = nn.Linear(h_dim, z_dim)
        self.fc2 = nn.Linear(h_dim, z_dim)
        self.fc3 = nn.Linear(z_dim, h_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Unflatten(1, (256, 2, 2)),
            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
            nn.Sigmoid(),
        )

    def reparameterize(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        esp = torch.randn(*mu.size())
        z = mu + std * esp
        return z

    def bottleneck(self, h):
        mu, logvar = self.fc1(h), self.fc2(h)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar

    def forward(self, x):
        h = self.encoder(x)
        z, mu, logvar = self.bottleneck(h)
        z = self.fc3(z)
        return self.decoder(z.view(-1, 256, 2, 2)), mu, logvar

# Loss components
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# VAE loss
def vae_loss(reconstructed_x, x, mu, log_var):
    BCE, KLD = vae_loss_components(reconstructed_x, x, mu, log_var)
    return BCE + KLD

# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_VAE().to(device)

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training Loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# # Visualizing Reconstructions
# import matplotlib.pyplot as plt

# with torch.no_grad():
#     sample = torch.randn(64, 256).to(device)  # change to match the latent_dim
#     sample = model.decoder(sample).cpu()

# fig = plt.figure(figsize=(10, 10))
# for i in range(64):
#     plt.subplot(8, 8, i + 1)
#     plt.imshow(sample[i].permute(1, 2, 0))  # change the order of dimensions to match the CIFAR-10 format
#     plt.axis("off")
# plt.show()

"""### Attemp 3:
- Enhanced Encoder and Decoder:
Added more convolutional layers to the encoder and decoder, providing a deeper architecture.
Introduced nn.BatchNorm2d layers after convolutional layers for normalization, which can stabilize training and accelerate convergence.
- Increased Model Depth:
The encoder now goes through three convolutional layers, transforming from 3 channels to 32, then to 64, and finally to 128 channels.
The decoder uses transposed convolutions to upsample and goes from 128 channels down to 3 channels in reverse order.
- Learning Rate Adjustment:
Reduced the learning rate to 0.0005 to potentially achieve more stable training.
- Regularization:
Introduced a weight decay of $1*10^{-5}$ in the Adam optimizer, adding L2 regularization to the model parameters.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

class CNNAE_VAE(nn.Module):
    def __init__(self):
        super(CNNAE_VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 32x16x16
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 64x8x8
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128x4x4
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 256x2x2
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(256*2*2, 512),
            nn.ReLU(),
            nn.Linear(512, 128)  # 64 for mean and 64 for log variance
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(64, 512),
            nn.ReLU(),
            nn.Linear(512, 256*2*2),
            nn.ReLU(),
            nn.Unflatten(1, (256, 2, 2)),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance)
        epsilon = torch.randn_like(std_dev)
        return mean + epsilon * std_dev

    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mean, log_variance)
        return self.decoder(z), mean, log_variance

# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNAE_VAE().to(device)

learning_rate = 0.0005
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

# Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# Visualizing Reconstructions
import matplotlib.pyplot as plt

with torch.no_grad():
    sample = torch.randn(64, 64).to(device)  # Adjust to match the latent_dim
    sample = model.decoder(sample).cpu()

fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].permute(1, 2, 0))  # change the order of dimensions to match the CIFAR-10 format
    plt.axis("off")
plt.show()

"""### Attempt 4:

- Increase network complexity.
- Introduce dropout for regularization.
- Add more batch normalization layers.
- Use LeakyReLU activations.

 - LeakyReLU is an activation function that allows a small, non-zero gradient when the input is less than zero, mitigating the "dying ReLU" problem. By permitting this tiny gradient for negative inputs, it ensures that neurons remain active and updates continue during training.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

class Improved_CNNAE_VAE(nn.Module):
    def __init__(self):
        super(Improved_CNNAE_VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.5),

            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.5),

            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.5),

            nn.Flatten(),
            nn.Linear(128*4*4, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 64)  # 32 for mean and 32 for log variance
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(32, 512),
            nn.LeakyReLU(0.2),

            nn.Linear(512, 128*4*4),
            nn.LeakyReLU(0.2),

            nn.Unflatten(1, (128, 4, 4)),

            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.5),

            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.5),

            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance)
        epsilon = torch.randn_like(std_dev)
        return mean + epsilon * std_dev

    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mean, log_variance)
        return self.decoder(z), mean, log_variance

# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Improved_CNNAE_VAE().to(device)

learning_rate = 0.0005
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

# Assuming vae_loss and vae_loss_components are defined as before...

# Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# Visualizing Reconstructions
import matplotlib.pyplot as plt

with torch.no_grad():
    # We only need half of the latent_dim for sampling since the other half is used for log variance
    sample = torch.randn(64, 32).to(device)
    sample = model.decoder(sample).cpu()

fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].permute(1, 2, 0))
    plt.axis("off")
plt.show()

"""### Attemp 5:
- Enhanced Encoder:
 - Increased the depth by adding more convolutional layers.
 - Introduced BatchNorm2d after most convolutional layers to stabilize and speed up training.
 - Replaced ReLU activations with LeakyReLU with a negative slope of 0.2 for better gradient flow during backpropagation.
 - Added a larger fully connected (dense) layer before the latent space to allow the model to process more complex representations.

- Enhanced Decoder:
 - Increased depth with more ConvTranspose2d layers to match the encoder's depth.
 - Introduced BatchNorm2d after most transposed convolutional layers.
 - Used LeakyReLU activations for better gradient propagation.
 - Increased the size of the initial dense layers to capture more complex reconstructions.

These modifications aim to capture more intricate patterns in the CIFAR-10 dataset and improve the reconstruction quality.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

class CNN_VAE(nn.Module):
    def __init__(self):
        super(CNN_VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            nn.Flatten(),
            nn.Linear(256*2*2, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 64)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(32, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256*2*2),
            nn.LeakyReLU(0.2),
            nn.Unflatten(1, (256, 2, 2)),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def reparameterize(self, mean, log_variance):
        std_dev = torch.exp(0.5 * log_variance)
        epsilon = torch.randn_like(std_dev)
        return mean + epsilon * std_dev

    def forward(self, x):
        h = self.encoder(x)
        mean, log_variance = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mean, log_variance)
        return self.decoder(z), mean, log_variance
# Data Loading
transform = transforms.ToTensor()
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model and optimizer setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_VAE().to(device)

learning_rate = 0.0005
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

# Loss components
def vae_loss_components(reconstructed_x, x, mu, log_var):
    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE, KLD

# VAE loss
def vae_loss(reconstructed_x, x, mu, log_var):
    BCE, KLD = vae_loss_components(reconstructed_x, x, mu, log_var)
    return BCE + KLD


# Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        reconstructed, mu, log_var = model(data)
        loss = vae_loss(reconstructed, data, mu, log_var)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}")

# Evaluation
model.eval()
total_test_loss = 0
total_BCE = 0
total_KLD = 0

with torch.no_grad():
    for data, _ in test_loader:
        data = data.to(device)
        reconstructed, mu, log_var = model(data)

        BCE, KLD = vae_loss_components(reconstructed, data, mu, log_var)
        total_BCE += BCE.item()
        total_KLD += KLD.item()
        total_test_loss += (BCE + KLD).item()

print(f"Total Test Loss: {total_test_loss / len(test_loader.dataset):.4f}")
print(f"Reconstruction Loss (BCE): {total_BCE / len(test_loader.dataset):.4f}")
print(f"KL Divergence (KLD): {total_KLD / len(test_loader.dataset):.4f}")

# Visualizing Reconstructions
import matplotlib.pyplot as plt

with torch.no_grad():
    # We only need half of the latent_dim for sampling since the other half is used for log variance
    sample = torch.randn(64, 32).to(device)
    sample = model.decoder(sample).cpu()

fig = plt.figure(figsize=(10, 10))
for i in range(64):
    plt.subplot(8, 8, i + 1)
    plt.imshow(sample[i].permute(1, 2, 0))
    plt.axis("off")
plt.show()

"""Epoch [1/10], Loss: 1886.9268

Epoch [2/10], Loss: 1848.1171

Epoch [3/10], Loss: 1842.6288

Epoch [4/10], Loss: 1838.6151

Epoch [5/10], Loss: 1835.6005

Epoch [6/10], Loss: 1833.6326

Epoch [7/10], Loss: 1832.4027

Epoch [8/10], Loss: 1831.0447

Epoch [9/10], Loss: 1829.7649

Epoch [10/10], Loss: 1829.2763

Total Test Loss: 1831.0373

Reconstruction Loss (BCE): 1797.9412

KL Divergence (KLD): 33.0961
"""



"""## 2.3  Common VAE variants and models


- **Vanilla VAE**: The basic VAE model and foundation for many variants.

- **Conditional VAE (CVAE)**: Allows generation of data given certain conditions, different from the standard VAE.

- **Beta-VAE**: A regularized VAE where the regularization strength in latent space can be controlled with a hyperparameter β.

- **InfoVAE**: Uses Maximum Mean Discrepancy (MMD) as a regularization term.

- **FactorVAE**: Aims to ensure learned latent variables are disentangled or independent from each other.

- **VQ-VAE (Vector Quantized VAE)**: Uses vector quantization for discrete encoding in latent space.

- **IWAE (Importance Weighted Autoencoder)**: Uses multiple samples to estimate the Evidence Lower BOund (ELBO), improving variational inference.

- **LVAE (Ladder VAE)**: Combines hierarchical latent variables with deterministic hierarchical connections for enhanced performance.

- **VampVAE**: Improves VAE performance by using a mixture prior.

- **DIP-VAE**: Ensures the encoder produces latent codes with desired independence properties.

- **Beta-TCVAE**: Amplifies the learning of causal factors.

- **WAE (Wasserstein Autoencoder)**: A VAE variant that uses the Wasserstein distance as its loss function.

# 2.3.1 Vanilla VAE

The Vanilla VAE is the foundational model of Variational Autoencoders (VAEs). A VAE consists of two parts: an encoder and a decoder. The encoder maps input data to a latent space, typically a lower-dimensional continuous space. The decoder samples from this latent space and reconstructs the original input. A unique aspect of VAEs is their training objective, which aims to minimize both the reconstruction error and the Kullback-Leibler (KL) divergence in the latent space. This ensures the latent space has good continuity and interpretability.


(Auto-Encoding Variational Bayes. Kingma, Diederik P ; Welling, Max. arXiv:1312.6114 [stat.ML]. 2013, https://doi.org/10.48550/arXiv.1312.6114

Stochastic Backpropagation and Approximate Inference in Deep Generative Models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra. 2014, arXiv:1401.4082.
https://doi.org/10.48550/arXiv.1401.4082)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision import datasets
import torch.optim as optim

# Define the VAE model
class VanillaVAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super(VanillaVAE, self).__init__()

        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # mu
        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # logvar

        # Decoder
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# Loss function
def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# Hyperparameters
batch_size = 64
learning_rate = 0.001
num_epochs = 5

# DataLoader
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Model, Optimizer
model = VanillaVAE().cuda()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.cuda()
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    print(f"Epoch: {epoch+1}, Loss: {train_loss / len(train_loader.dataset)}")

import matplotlib.pyplot as plt

# Model Evaluation
def evaluate(model, test_loader):
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for data, _ in test_loader:
            data = data.cuda()
            recon_batch, mu, logvar = model(data)
            test_loss += loss_function(recon_batch, data, mu, logvar).item()

    test_loss /= len(test_loader.dataset)
    return test_loss

test_loss = evaluate(model, test_loader)
print(f"Test Loss: {test_loss}")

# Visualizing Reconstructions
def visualize_reconstructions(model, data_loader, num_samples=64):
    model.eval()
    dataiter = iter(data_loader)
    images, _ = next(dataiter)  # Corrected this line
    images = images[:num_samples].cuda()
    with torch.no_grad():
        reconstructions, _, _ = model(images)

    images = images.cpu().numpy().reshape(-1, 28, 28)
    reconstructions = reconstructions.cpu().numpy().reshape(-1, 28, 28)

    fig, axes = plt.subplots(nrows=8, ncols=8, figsize=(10, 4))
    for i in range(8):
        for j in range(8):
            idx = i * 8 + j
            if idx < num_samples:
                axes[i, j].imshow(images[idx], cmap='gray')
                axes[i, j].axis('off')
            # If you want to place reconstructions side by side, you can modify this part accordingly.
    plt.tight_layout()
    plt.show()

visualize_reconstructions(model, test_loader)













